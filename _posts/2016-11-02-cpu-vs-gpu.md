---
title: CPU vs GPU for deep learning
tags:
- TensorFlow
- AWS
---
If anyone is wondering why would you need to use AWS for machine learning after reading [this post](http://navoshta.com/aws-tensorflow/), here's a real example. I've tried training the same model with the same data on CPU of my MacBook Pro (2.5 GHz Intel Core i7) and GPU on a AWS instance (g2.xlarge).<!--more-->

<figure class="align-left" style="width: 300px">
  <img src="{{ base_path }}/images/posts/gpu_vs_cpu-aws.png_" alt="GPU-enabled AWS instance.">
  <figcaption>GPU-enabled AWS instance.</figcaption>
</figure> 

<figure class="align-left" style="width: 300px">
  <img src="{{ base_path }}/images/posts/gpu_vs_cpu-mac.png_" alt="MacBook Pro CPU (2.5 GHz Intel Core i7)">
  <figcaption>MacBook Pro CPU (2.5 GHz Intel Core i7)</figcaption>
</figure> 

The model here is a **CNN** with **3 convolutional layers** and **2 fully connected layers**, implemented with TensorFlow. It was trained on **~1500 grayscale 96x96 images in batches of 36 for 1000 epochs**. As you see, in this case GPU was approximately 10 times faster to train than a CPU. That's a huge difference!

To be fair, if I could use my MacBook Pro's GPU, the results should be roughly in the same ballpark. Unfortunately it's equipped with AMD GPU that has a OpenCL interface, and TensorFlow is [yet to support OpenCL](https://github.com/tensorflow/tensorflow/issues/22).